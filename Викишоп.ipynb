{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Ivan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/Ivan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Ivan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Ivan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import logging\n",
    "from pymystem3 import Mystem\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "logging.basicConfig(level=logging.NOTSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Импорт данных\n",
    "\n",
    "try:\n",
    "    toxic_comments = pd.read_csv('/Users/Ivan/toxic_comments.csv')\n",
    "except:\n",
    "    toxic_comments = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')\n",
    "\n",
    "toxic_comments.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "toxic_comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(toxic_comments['toxic'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Вывод</b>\n",
    "\n",
    "Классы несбалансированы. Необходимо учитывать это при обучении моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для лемматизации и очистки текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция лемматизации и удаления лишних символов\n",
    "\n",
    "def clear_text(text):\n",
    "    re_list = re.sub(r\"[^a-zA-Z']\", ' ', text)\n",
    "    re_list = re_list.split()\n",
    "    re_list = \" \".join(re_list)\n",
    "    return re_list\n",
    "\n",
    "m = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    \n",
    "    return ' '.join([m.lemmatize(w) for w in word_list])\n",
    "\n",
    "def get_wordnet_pos(text):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([text])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_comments['text'] = toxic_comments['text'].apply(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_comments['clean_text'] = toxic_comments['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_comments['lemm_text'] = toxic_comments['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww He matches this background colour I'm se...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he matches this background colour i am s...</td>\n",
       "      <td>D'aww He match this background colour I 'm see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man I'm really not trying to edit war It's...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i am really not trying to edit war it ...</td>\n",
       "      <td>Hey man I 'm really not trying to edit war It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More I can't make any real suggestions on impr...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i cannot make any real suggestions on imp...</td>\n",
       "      <td>More I ca n't make any real suggestion on impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>And for the second time of asking when your vi...</td>\n",
       "      <td>0</td>\n",
       "      <td>and for the second time of asking when your vi...</td>\n",
       "      <td>And for the second time of asking when your vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself That is a ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>you should be ashamed of yourself that is a ho...</td>\n",
       "      <td>You should be ashamed of yourself That is a ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer Umm theres no actual article for prost...</td>\n",
       "      <td>0</td>\n",
       "      <td>spitzer umm theres no actual article for prost...</td>\n",
       "      <td>Spitzer Umm there no actual article for prosti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and it looks like it was actually you who put ...</td>\n",
       "      <td>And it look like it wa actually you who put on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>And I really don't think you understand I came...</td>\n",
       "      <td>0</td>\n",
       "      <td>and i really do not think you understand i cam...</td>\n",
       "      <td>And I really do n't think you understand I cam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "0       Explanation Why the edits made under my userna...      0   \n",
       "1       D'aww He matches this background colour I'm se...      0   \n",
       "2       Hey man I'm really not trying to edit war It's...      0   \n",
       "3       More I can't make any real suggestions on impr...      0   \n",
       "4       You sir are my hero Any chance you remember wh...      0   \n",
       "...                                                   ...    ...   \n",
       "159566  And for the second time of asking when your vi...      0   \n",
       "159567  You should be ashamed of yourself That is a ho...      0   \n",
       "159568  Spitzer Umm theres no actual article for prost...      0   \n",
       "159569  And it looks like it was actually you who put ...      0   \n",
       "159570  And I really don't think you understand I came...      0   \n",
       "\n",
       "                                               clean_text  \\\n",
       "0       explanation why the edits made under my userna...   \n",
       "1       d aww he matches this background colour i am s...   \n",
       "2       hey man i am really not trying to edit war it ...   \n",
       "3       more i cannot make any real suggestions on imp...   \n",
       "4       you sir are my hero any chance you remember wh...   \n",
       "...                                                   ...   \n",
       "159566  and for the second time of asking when your vi...   \n",
       "159567  you should be ashamed of yourself that is a ho...   \n",
       "159568  spitzer umm theres no actual article for prost...   \n",
       "159569  and it looks like it was actually you who put ...   \n",
       "159570  and i really do not think you understand i cam...   \n",
       "\n",
       "                                                lemm_text  \n",
       "0       Explanation Why the edits made under my userna...  \n",
       "1       D'aww He match this background colour I 'm see...  \n",
       "2       Hey man I 'm really not trying to edit war It ...  \n",
       "3       More I ca n't make any real suggestion on impr...  \n",
       "4       You sir are my hero Any chance you remember wh...  \n",
       "...                                                   ...  \n",
       "159566  And for the second time of asking when your vi...  \n",
       "159567  You should be ashamed of yourself That is a ho...  \n",
       "159568  Spitzer Umm there no actual article for prosti...  \n",
       "159569  And it look like it wa actually you who put on...  \n",
       "159570  And I really do n't think you understand I cam...  \n",
       "\n",
       "[159571 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_comments = toxic_comments.drop(['text'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим корпус из лематизированных и очищеных тексов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = toxic_comments['lemm_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpuss = toxic_comments['clean_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "соотношение классов в датасете corpuss\n",
      " 0    89.832112\n",
      "1    10.167888\n",
      "Name: toxic, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "corpuss = toxic_comments.reset_index(drop=True)\n",
    "print('соотношение классов в датасете corpuss\\n', corpuss.toxic.value_counts()/corpuss.shape[0]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализация Wordnet Lemmatizer\n",
    "L = WordNetLemmatizer()\n",
    "def get_word_text(corpuss):\n",
    "    ''' функция выполняет токенизациию и лемматизацию массива текстов c учетом pos_tag и удаление стоп-слов'''\n",
    "    corpus_new = []\n",
    "    for sentence in corpuss:\n",
    "        corpus_new.append(' '.join([L.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence) if not w in stopwords.words('english')]))\n",
    "    return corpus_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpuss['lemma_text_no_sw'] = get_word_text(corpuss['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemm_text</th>\n",
       "      <th>lemma_text_no_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>explanation edits make username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>d aww he matches this background colour i am s...</td>\n",
       "      <td>D'aww He match this background colour I 'm see...</td>\n",
       "      <td>aww match background colour seemingly stuck th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>hey man i am really not trying to edit war it ...</td>\n",
       "      <td>Hey man I 'm really not trying to edit war It ...</td>\n",
       "      <td>hey man really try edit war guy constantly rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>more i cannot make any real suggestions on imp...</td>\n",
       "      <td>More I ca n't make any real suggestion on impr...</td>\n",
       "      <td>make real suggestion improvement wonder sectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic                                         clean_text  \\\n",
       "0      0  explanation why the edits made under my userna...   \n",
       "1      0  d aww he matches this background colour i am s...   \n",
       "2      0  hey man i am really not trying to edit war it ...   \n",
       "3      0  more i cannot make any real suggestions on imp...   \n",
       "4      0  you sir are my hero any chance you remember wh...   \n",
       "\n",
       "                                           lemm_text  \\\n",
       "0  Explanation Why the edits made under my userna...   \n",
       "1  D'aww He match this background colour I 'm see...   \n",
       "2  Hey man I 'm really not trying to edit war It ...   \n",
       "3  More I ca n't make any real suggestion on impr...   \n",
       "4  You sir are my hero Any chance you remember wh...   \n",
       "\n",
       "                                    lemma_text_no_sw  \n",
       "0  explanation edits make username hardcore metal...  \n",
       "1  aww match background colour seemingly stuck th...  \n",
       "2  hey man really try edit war guy constantly rem...  \n",
       "3  make real suggestion improvement wonder sectio...  \n",
       "4                      sir hero chance remember page  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpuss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpuss['lemma_text_no_sw'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = corpus\n",
    "target = toxic_comments['toxic'].values\n",
    "\n",
    "train_features, test_features, train_target, test_target = train_test_split(features, target, test_size=0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordss = set(stopwords.words('english'))\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words = stopwordss)\n",
    "train_features = count_tf_idf.fit_transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = count_tf_idf.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>LogisticRegression</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели:\n",
      "\n",
      "{'C': 10, 'class_weight': 'balanced'}\n",
      "\n",
      "F1: 0.7566809701504811\n",
      "\n",
      "CPU times: user 58.3 s, sys: 636 ms, total: 58.9 s\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_model = LogisticRegression()\n",
    "hyperparams = [{'C':[10],   # так же подбирал [0.1, 1, 3]\n",
    "                'class_weight':['balanced']}]\n",
    "clf = GridSearchCV(lr_model, hyperparams, scoring='f1',cv=3)\n",
    "clf.fit(train_features, train_target)\n",
    "print(\"Лучшие параметры модели:\")\n",
    "print()\n",
    "LR_best_params = clf.best_params_\n",
    "print(LR_best_params)\n",
    "print()\n",
    "print('F1:', clf.best_score_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 регрессии: 0.757473209249859\n",
      "\n",
      "Матрица ошибок\n",
      "[[27509  1167]\n",
      " [  553  2686]]\n",
      "\n",
      "CPU times: user 17.1 s, sys: 179 ms, total: 17.3 s\n",
      "Wall time: 4.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.set_params(**LR_best_params)\n",
    "lr_model.fit(train_features, train_target)\n",
    "prediction = lr_model.predict(test_features)\n",
    "f1 = f1_score(test_target, prediction)\n",
    "print('F1 регрессии:', f1)\n",
    "print()\n",
    "print('Матрица ошибок')\n",
    "print(confusion_matrix(test_target, prediction))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>CatBoostClassifier</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2638740\ttotal: 5.14s\tremaining: 8m 28s\n",
      "50:\tlearn: 0.1120608\ttotal: 1m 24s\tremaining: 1m 21s\n",
      "99:\tlearn: 0.0901090\ttotal: 2m 27s\tremaining: 0us\n",
      "0:\tlearn: 0.2664615\ttotal: 2.04s\tremaining: 3m 21s\n",
      "50:\tlearn: 0.1114486\ttotal: 2m 17s\tremaining: 2m 12s\n",
      "99:\tlearn: 0.0884690\ttotal: 4m 33s\tremaining: 0us\n",
      "0:\tlearn: 0.2591023\ttotal: 3.33s\tremaining: 5m 29s\n",
      "50:\tlearn: 0.1134895\ttotal: 1m 41s\tremaining: 1m 37s\n",
      "99:\tlearn: 0.0903896\ttotal: 3m 34s\tremaining: 0us\n",
      "0:\tlearn: 0.2582263\ttotal: 5s\tremaining: 8m 15s\n",
      "50:\tlearn: 0.1149256\ttotal: 6m 50s\tremaining: 6m 34s\n",
      "99:\tlearn: 0.0971054\ttotal: 15m 18s\tremaining: 0us\n",
      "Лучшие параметры модели:\n",
      "\n",
      "{'iterations': 100, 'learning_rate': 0.9, 'max_depth': 8, 'random_state': 12345}\n",
      "\n",
      "F1: 0.7437599400244449\n",
      "CPU times: user 1h 32min 14s, sys: 29min 5s, total: 2h 1min 19s\n",
      "Wall time: 26min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cat_model = CatBoostClassifier()\n",
    "hyperparams = [{'max_depth' : [8], # 4, 6 - f1=0.0.7414 , 8 - f1=0.0.744, 10 - f1=0.73\n",
    "                'learning_rate':[0.9], # 0.03, 0.1, 0.3, 0.9\n",
    "                'iterations':[100], #100\n",
    "                'random_state':[12345]}] # 12345, 43\n",
    "clf = GridSearchCV(cat_model, hyperparams, scoring='f1',cv=3) # 3, 2\n",
    "clf.fit(train_features, train_target, verbose=50) #20, 50 (лучшая f1), 100\n",
    "print(\"Лучшие параметры модели:\")\n",
    "print()\n",
    "CAT_best_params = clf.best_params_\n",
    "print(CAT_best_params)\n",
    "print()\n",
    "print('F1:', clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.2582263\ttotal: 7.49s\tremaining: 12m 21s\n",
      "1:\tlearn: 0.2229088\ttotal: 15.4s\tremaining: 12m 35s\n",
      "2:\tlearn: 0.2034951\ttotal: 23.5s\tremaining: 12m 38s\n",
      "3:\tlearn: 0.1931086\ttotal: 31.2s\tremaining: 12m 29s\n",
      "4:\tlearn: 0.1859810\ttotal: 38.9s\tremaining: 12m 19s\n",
      "5:\tlearn: 0.1803156\ttotal: 47s\tremaining: 12m 15s\n",
      "6:\tlearn: 0.1744803\ttotal: 54.8s\tremaining: 12m 7s\n",
      "7:\tlearn: 0.1697633\ttotal: 1m 2s\tremaining: 11m 59s\n",
      "8:\tlearn: 0.1657452\ttotal: 1m 10s\tremaining: 11m 47s\n",
      "9:\tlearn: 0.1624250\ttotal: 1m 17s\tremaining: 11m 35s\n",
      "10:\tlearn: 0.1601682\ttotal: 1m 25s\tremaining: 11m 29s\n",
      "11:\tlearn: 0.1561786\ttotal: 1m 32s\tremaining: 11m 16s\n",
      "12:\tlearn: 0.1533869\ttotal: 1m 39s\tremaining: 11m 6s\n",
      "13:\tlearn: 0.1512084\ttotal: 1m 47s\tremaining: 10m 58s\n",
      "14:\tlearn: 0.1490677\ttotal: 1m 56s\tremaining: 10m 58s\n",
      "15:\tlearn: 0.1472815\ttotal: 2m 4s\tremaining: 10m 51s\n",
      "16:\tlearn: 0.1446842\ttotal: 2m 12s\tremaining: 10m 48s\n",
      "17:\tlearn: 0.1430311\ttotal: 2m 22s\tremaining: 10m 48s\n",
      "18:\tlearn: 0.1415993\ttotal: 2m 30s\tremaining: 10m 41s\n",
      "19:\tlearn: 0.1396496\ttotal: 2m 39s\tremaining: 10m 36s\n",
      "20:\tlearn: 0.1386497\ttotal: 2m 48s\tremaining: 10m 32s\n",
      "21:\tlearn: 0.1375601\ttotal: 2m 57s\tremaining: 10m 28s\n",
      "22:\tlearn: 0.1358672\ttotal: 3m 5s\tremaining: 10m 19s\n",
      "23:\tlearn: 0.1350291\ttotal: 3m 13s\tremaining: 10m 14s\n",
      "24:\tlearn: 0.1337114\ttotal: 3m 22s\tremaining: 10m 7s\n",
      "25:\tlearn: 0.1318968\ttotal: 3m 30s\tremaining: 9m 59s\n",
      "26:\tlearn: 0.1311532\ttotal: 3m 38s\tremaining: 9m 50s\n",
      "27:\tlearn: 0.1301932\ttotal: 3m 45s\tremaining: 9m 41s\n",
      "28:\tlearn: 0.1295912\ttotal: 3m 53s\tremaining: 9m 32s\n",
      "29:\tlearn: 0.1288899\ttotal: 4m 1s\tremaining: 9m 22s\n",
      "30:\tlearn: 0.1281375\ttotal: 4m 10s\tremaining: 9m 16s\n",
      "31:\tlearn: 0.1271878\ttotal: 4m 18s\tremaining: 9m 8s\n",
      "32:\tlearn: 0.1267009\ttotal: 4m 26s\tremaining: 9m\n",
      "33:\tlearn: 0.1256599\ttotal: 4m 34s\tremaining: 8m 52s\n",
      "34:\tlearn: 0.1248745\ttotal: 4m 42s\tremaining: 8m 43s\n",
      "35:\tlearn: 0.1242878\ttotal: 4m 49s\tremaining: 8m 35s\n",
      "36:\tlearn: 0.1228692\ttotal: 4m 58s\tremaining: 8m 27s\n",
      "37:\tlearn: 0.1224149\ttotal: 5m 5s\tremaining: 8m 18s\n",
      "38:\tlearn: 0.1216164\ttotal: 5m 13s\tremaining: 8m 10s\n",
      "39:\tlearn: 0.1205675\ttotal: 5m 21s\tremaining: 8m 2s\n",
      "40:\tlearn: 0.1199148\ttotal: 5m 28s\tremaining: 7m 53s\n",
      "41:\tlearn: 0.1195150\ttotal: 5m 36s\tremaining: 7m 44s\n",
      "42:\tlearn: 0.1190690\ttotal: 5m 44s\tremaining: 7m 37s\n",
      "43:\tlearn: 0.1186496\ttotal: 5m 52s\tremaining: 7m 28s\n",
      "44:\tlearn: 0.1181426\ttotal: 6m\tremaining: 7m 20s\n",
      "45:\tlearn: 0.1177045\ttotal: 6m 8s\tremaining: 7m 12s\n",
      "46:\tlearn: 0.1170609\ttotal: 6m 17s\tremaining: 7m 5s\n",
      "47:\tlearn: 0.1165619\ttotal: 6m 25s\tremaining: 6m 57s\n",
      "48:\tlearn: 0.1158629\ttotal: 6m 33s\tremaining: 6m 49s\n",
      "49:\tlearn: 0.1155080\ttotal: 6m 41s\tremaining: 6m 41s\n",
      "50:\tlearn: 0.1149256\ttotal: 6m 49s\tremaining: 6m 33s\n",
      "51:\tlearn: 0.1146162\ttotal: 6m 57s\tremaining: 6m 25s\n",
      "52:\tlearn: 0.1142760\ttotal: 7m 5s\tremaining: 6m 17s\n",
      "53:\tlearn: 0.1135943\ttotal: 7m 14s\tremaining: 6m 9s\n",
      "54:\tlearn: 0.1132070\ttotal: 7m 21s\tremaining: 6m 1s\n",
      "55:\tlearn: 0.1127927\ttotal: 7m 30s\tremaining: 5m 53s\n",
      "56:\tlearn: 0.1124555\ttotal: 7m 38s\tremaining: 5m 46s\n",
      "57:\tlearn: 0.1121455\ttotal: 7m 46s\tremaining: 5m 37s\n",
      "58:\tlearn: 0.1118402\ttotal: 7m 54s\tremaining: 5m 29s\n",
      "59:\tlearn: 0.1115522\ttotal: 8m 2s\tremaining: 5m 21s\n",
      "60:\tlearn: 0.1112723\ttotal: 8m 10s\tremaining: 5m 13s\n",
      "61:\tlearn: 0.1109145\ttotal: 8m 17s\tremaining: 5m 5s\n",
      "62:\tlearn: 0.1100607\ttotal: 8m 26s\tremaining: 4m 57s\n",
      "63:\tlearn: 0.1097774\ttotal: 8m 32s\tremaining: 4m 48s\n",
      "64:\tlearn: 0.1095198\ttotal: 8m 40s\tremaining: 4m 40s\n",
      "65:\tlearn: 0.1092399\ttotal: 8m 47s\tremaining: 4m 31s\n",
      "66:\tlearn: 0.1088907\ttotal: 8m 55s\tremaining: 4m 23s\n",
      "67:\tlearn: 0.1085113\ttotal: 9m 3s\tremaining: 4m 15s\n",
      "68:\tlearn: 0.1082554\ttotal: 9m 10s\tremaining: 4m 7s\n",
      "69:\tlearn: 0.1080138\ttotal: 9m 18s\tremaining: 3m 59s\n",
      "70:\tlearn: 0.1077861\ttotal: 9m 26s\tremaining: 3m 51s\n",
      "71:\tlearn: 0.1075621\ttotal: 9m 34s\tremaining: 3m 43s\n",
      "72:\tlearn: 0.1071838\ttotal: 9m 44s\tremaining: 3m 36s\n",
      "73:\tlearn: 0.1068826\ttotal: 9m 52s\tremaining: 3m 28s\n",
      "74:\tlearn: 0.1063709\ttotal: 10m 1s\tremaining: 3m 20s\n",
      "75:\tlearn: 0.1057870\ttotal: 10m 10s\tremaining: 3m 12s\n",
      "76:\tlearn: 0.1049634\ttotal: 10m 17s\tremaining: 3m 4s\n",
      "77:\tlearn: 0.1047089\ttotal: 10m 26s\tremaining: 2m 56s\n",
      "78:\tlearn: 0.1040211\ttotal: 10m 34s\tremaining: 2m 48s\n",
      "79:\tlearn: 0.1037446\ttotal: 10m 41s\tremaining: 2m 40s\n",
      "80:\tlearn: 0.1035252\ttotal: 10m 48s\tremaining: 2m 32s\n",
      "81:\tlearn: 0.1032672\ttotal: 10m 57s\tremaining: 2m 24s\n",
      "82:\tlearn: 0.1026883\ttotal: 11m 5s\tremaining: 2m 16s\n",
      "83:\tlearn: 0.1021715\ttotal: 11m 12s\tremaining: 2m 8s\n",
      "84:\tlearn: 0.1019500\ttotal: 11m 20s\tremaining: 2m\n",
      "85:\tlearn: 0.1012863\ttotal: 11m 27s\tremaining: 1m 51s\n",
      "86:\tlearn: 0.1010666\ttotal: 11m 34s\tremaining: 1m 43s\n",
      "87:\tlearn: 0.1008607\ttotal: 11m 41s\tremaining: 1m 35s\n",
      "88:\tlearn: 0.1003432\ttotal: 11m 49s\tremaining: 1m 27s\n",
      "89:\tlearn: 0.0998039\ttotal: 11m 57s\tremaining: 1m 19s\n",
      "90:\tlearn: 0.0995947\ttotal: 12m 4s\tremaining: 1m 11s\n",
      "91:\tlearn: 0.0994122\ttotal: 12m 12s\tremaining: 1m 3s\n",
      "92:\tlearn: 0.0991800\ttotal: 12m 20s\tremaining: 55.7s\n",
      "93:\tlearn: 0.0989383\ttotal: 12m 28s\tremaining: 47.8s\n",
      "94:\tlearn: 0.0987457\ttotal: 12m 36s\tremaining: 39.8s\n",
      "95:\tlearn: 0.0985819\ttotal: 12m 43s\tremaining: 31.8s\n",
      "96:\tlearn: 0.0983967\ttotal: 12m 51s\tremaining: 23.9s\n",
      "97:\tlearn: 0.0979752\ttotal: 12m 58s\tremaining: 15.9s\n",
      "98:\tlearn: 0.0972940\ttotal: 13m 6s\tremaining: 7.95s\n",
      "99:\tlearn: 0.0971054\ttotal: 13m 14s\tremaining: 0us\n",
      "F1 0.7537859452101412\n",
      "\n",
      "Матрица ошибок\n",
      "[[28253   423]\n",
      " [ 1024  2215]]\n",
      "\n",
      "CPU times: user 38min 1s, sys: 21min 17s, total: 59min 18s\n",
      "Wall time: 13min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cat_model = CatBoostClassifier()\n",
    "cat_model.set_params(**CAT_best_params)\n",
    "cat_model.fit(train_features, train_target)\n",
    "prediction = cat_model.predict(test_features)\n",
    "f1 = f1_score(test_target, prediction)\n",
    "print('F1', f1)\n",
    "print()\n",
    "print('Матрица ошибок')\n",
    "print(confusion_matrix(test_target, prediction))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>LightGBM model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели:\n",
      "\n",
      "{'learning_rate': 0.3, 'max_depth': -1, 'n_estimators': 500, 'random_state': 12345}\n",
      "\n",
      "F1: 0.7652294240478755\n",
      "\n",
      "CPU times: user 32min 10s, sys: 21.3 s, total: 32min 32s\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# подбирал параметры кросс валидацией\n",
    "\n",
    "LightGBM_model = LGBMClassifier()\n",
    "hyperparams = [{'max_depth' : [-1], # -1, 1\n",
    "                'learning_rate':[0.3], # 0.03, 0.1, 0.3\n",
    "                'n_estimators' : [500],  # 200, 500, 1000\n",
    "                'random_state':[12345]}]\n",
    "clf = GridSearchCV(LightGBM_model, hyperparams, scoring='f1',cv=3)\n",
    "clf.fit(train_features, train_target, verbose=20)\n",
    "print(\"Лучшие параметры модели:\")\n",
    "print()\n",
    "LGBM_best_params = clf.best_params_\n",
    "print(LGBM_best_params)\n",
    "print()\n",
    "print('F1:', clf.best_score_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.7794663964876731\n",
      "\n",
      "Матрица ошибок\n",
      "[[28301   375]\n",
      " [  931  2308]]\n",
      "\n",
      "CPU times: user 9min 56s, sys: 6.51 s, total: 10min 3s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "LightGBM_model = LGBMClassifier()\n",
    "LightGBM_model.set_params(**LGBM_best_params)\n",
    "LightGBM_model.fit(train_features, train_target)\n",
    "prediction = LightGBM_model.predict(test_features)\n",
    "f1 = f1_score(test_target, prediction)\n",
    "print('F1:', f1)\n",
    "print()\n",
    "print('Матрица ошибок')\n",
    "print(confusion_matrix(test_target, prediction))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все модели имеют удовлетворительное значение F1. \n",
    "\n",
    "Лучше всех справилась LightGBM model, но у LogisticRegression меньше ложно отрицательных предсказаний, значит меньше токсичных коментариев пройдут мимо модерации. \n",
    "\n",
    "То что у LogisticRegression больше ложно положительных предсказаний не страшно, так как после модерации они вернуться обратно.\n",
    "\n",
    "Исходя из этого, для бизнеса, лучше подходит модель LogisticRegression"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 11800,
    "start_time": "2022-07-11T12:28:01.200Z"
   },
   {
    "duration": 14001,
    "start_time": "2022-07-11T12:29:05.800Z"
   },
   {
    "duration": 201,
    "start_time": "2022-07-11T12:29:37.999Z"
   },
   {
    "duration": 12,
    "start_time": "2022-07-11T12:29:50.699Z"
   },
   {
    "duration": 102,
    "start_time": "2022-07-11T12:30:51.100Z"
   },
   {
    "duration": 4,
    "start_time": "2022-07-11T12:31:37.900Z"
   },
   {
    "duration": 25014,
    "start_time": "2022-07-11T12:31:58.898Z"
   },
   {
    "duration": 0,
    "start_time": "2022-07-11T12:36:54.014Z"
   },
   {
    "duration": 4,
    "start_time": "2022-07-11T12:45:52.401Z"
   },
   {
    "duration": 17416,
    "start_time": "2022-07-11T12:47:11.198Z"
   },
   {
    "duration": 285257,
    "start_time": "2022-07-11T12:47:51.099Z"
   },
   {
    "duration": 12,
    "start_time": "2022-07-11T12:52:36.358Z"
   },
   {
    "duration": 33,
    "start_time": "2022-07-11T12:53:36.844Z"
   },
   {
    "duration": 3,
    "start_time": "2022-07-11T12:54:11.539Z"
   },
   {
    "duration": 20,
    "start_time": "2022-07-11T12:54:51.417Z"
   },
   {
    "duration": 5825,
    "start_time": "2022-07-11T12:55:36.091Z"
   },
   {
    "duration": 1378,
    "start_time": "2022-07-11T12:55:52.663Z"
   },
   {
    "duration": 15,
    "start_time": "2022-07-11T12:56:08.395Z"
   },
   {
    "duration": 4,
    "start_time": "2022-07-11T12:59:15.069Z"
   },
   {
    "duration": 174892,
    "start_time": "2022-07-11T13:00:40.008Z"
   },
   {
    "duration": 52,
    "start_time": "2022-07-11T13:05:05.550Z"
   },
   {
    "duration": 8,
    "start_time": "2022-07-11T13:05:28.155Z"
   },
   {
    "duration": 2832,
    "start_time": "2022-07-11T13:05:37.120Z"
   },
   {
    "duration": 85,
    "start_time": "2022-07-11T13:05:49.259Z"
   },
   {
    "duration": 11,
    "start_time": "2022-07-11T13:05:56.450Z"
   },
   {
    "duration": 3498,
    "start_time": "2022-07-11T13:06:10.444Z"
   },
   {
    "duration": 35,
    "start_time": "2022-07-11T13:06:15.842Z"
   },
   {
    "duration": 6,
    "start_time": "2022-07-11T13:06:17.898Z"
   },
   {
    "duration": 4,
    "start_time": "2022-07-11T13:06:20.902Z"
   },
   {
    "duration": 4024,
    "start_time": "2022-07-11T13:06:24.301Z"
   },
   {
    "duration": 84038,
    "start_time": "2022-07-11T13:06:29.270Z"
   },
   {
    "duration": 14,
    "start_time": "2022-07-11T13:09:16.043Z"
   },
   {
    "duration": 31,
    "start_time": "2022-07-11T13:09:20.026Z"
   },
   {
    "duration": 2,
    "start_time": "2022-07-11T13:09:22.794Z"
   },
   {
    "duration": 28,
    "start_time": "2022-07-11T13:09:25.621Z"
   },
   {
    "duration": 5886,
    "start_time": "2022-07-11T13:09:28.466Z"
   },
   {
    "duration": 1415,
    "start_time": "2022-07-11T13:09:36.560Z"
   },
   {
    "duration": 54,
    "start_time": "2022-07-11T13:10:18.958Z"
   },
   {
    "duration": 6,
    "start_time": "2022-07-11T13:10:28.692Z"
   },
   {
    "duration": 2729,
    "start_time": "2022-07-11T13:11:41.086Z"
   },
   {
    "duration": 3565,
    "start_time": "2022-07-11T13:11:44.532Z"
   },
   {
    "duration": 35,
    "start_time": "2022-07-11T13:11:48.832Z"
   },
   {
    "duration": 7,
    "start_time": "2022-07-11T13:11:54.097Z"
   },
   {
    "duration": 4,
    "start_time": "2022-07-11T13:11:56.666Z"
   },
   {
    "duration": 3912,
    "start_time": "2022-07-11T13:11:56.849Z"
   },
   {
    "duration": 84453,
    "start_time": "2022-07-11T13:12:01.850Z"
   },
   {
    "duration": 15,
    "start_time": "2022-07-11T13:13:38.921Z"
   },
   {
    "duration": 31,
    "start_time": "2022-07-11T13:13:42.624Z"
   },
   {
    "duration": 3,
    "start_time": "2022-07-11T13:13:44.146Z"
   },
   {
    "duration": 22,
    "start_time": "2022-07-11T13:13:45.658Z"
   },
   {
    "duration": 5803,
    "start_time": "2022-07-11T13:13:47.420Z"
   },
   {
    "duration": 180,
    "start_time": "2022-07-11T13:14:03.499Z"
   },
   {
    "duration": 173955,
    "start_time": "2022-07-11T13:14:44.845Z"
   },
   {
    "duration": 44784,
    "start_time": "2022-07-11T13:17:44.580Z"
   },
   {
    "duration": 577613,
    "start_time": "2022-07-11T13:19:42.375Z"
   },
   {
    "duration": 25194,
    "start_time": "2022-07-11T13:29:19.991Z"
   },
   {
    "duration": 1385,
    "start_time": "2022-07-11T13:29:45.186Z"
   },
   {
    "duration": 46381,
    "start_time": "2022-07-11T13:30:00.018Z"
   },
   {
    "duration": 191457,
    "start_time": "2022-07-11T13:31:17.522Z"
   },
   {
    "duration": 2045,
    "start_time": "2022-07-11T17:11:37.764Z"
   },
   {
    "duration": 3390,
    "start_time": "2022-07-11T17:11:57.497Z"
   },
   {
    "duration": 32,
    "start_time": "2022-07-11T17:13:03.893Z"
   },
   {
    "duration": 6,
    "start_time": "2022-07-11T17:13:07.026Z"
   },
   {
    "duration": 4,
    "start_time": "2022-07-11T17:13:14.554Z"
   },
   {
    "duration": 3495,
    "start_time": "2022-07-11T17:13:18.098Z"
   },
   {
    "duration": 73779,
    "start_time": "2022-07-11T17:13:26.504Z"
   },
   {
    "duration": 11,
    "start_time": "2022-07-11T17:14:44.629Z"
   },
   {
    "duration": 24,
    "start_time": "2022-07-11T17:15:52.420Z"
   },
   {
    "duration": 3,
    "start_time": "2022-07-11T17:15:54.413Z"
   },
   {
    "duration": 23,
    "start_time": "2022-07-11T17:15:56.096Z"
   },
   {
    "duration": 5228,
    "start_time": "2022-07-11T17:15:59.308Z"
   },
   {
    "duration": 1163,
    "start_time": "2022-07-11T17:16:09.421Z"
   },
   {
    "duration": 164296,
    "start_time": "2022-07-11T17:17:04.664Z"
   },
   {
    "duration": 41536,
    "start_time": "2022-07-11T17:20:21.232Z"
   },
   {
    "duration": 1957,
    "start_time": "2022-07-11T17:22:31.936Z"
   },
   {
    "duration": 3228,
    "start_time": "2022-07-11T17:22:33.895Z"
   },
   {
    "duration": 29,
    "start_time": "2022-07-11T17:22:37.124Z"
   },
   {
    "duration": 34,
    "start_time": "2022-07-11T17:22:37.155Z"
   },
   {
    "duration": 15,
    "start_time": "2022-07-11T17:22:37.191Z"
   },
   {
    "duration": 3520,
    "start_time": "2022-07-11T17:22:37.208Z"
   },
   {
    "duration": 72127,
    "start_time": "2022-07-11T17:22:40.729Z"
   },
   {
    "duration": 11,
    "start_time": "2022-07-11T17:23:52.858Z"
   },
   {
    "duration": 24,
    "start_time": "2022-07-11T17:23:52.871Z"
   },
   {
    "duration": 2,
    "start_time": "2022-07-11T17:23:52.897Z"
   },
   {
    "duration": 18,
    "start_time": "2022-07-11T17:23:52.901Z"
   },
   {
    "duration": 5060,
    "start_time": "2022-07-11T17:23:52.921Z"
   },
   {
    "duration": 1170,
    "start_time": "2022-07-11T17:23:57.983Z"
   },
   {
    "duration": 161200,
    "start_time": "2022-07-11T17:23:59.155Z"
   },
   {
    "duration": 42429,
    "start_time": "2022-07-11T17:26:40.357Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
